# Data Submission Tracker Deployment Guide
The Data Submission Tracker (DST) is a web-based tool for following the progress of Data Submissions through the NHLBI BioData Catalyst Data Management Core ingest pipeline, allowing self-service data submission and tracking. The tool is designed to be deployed on a cloud platform, such as Google Cloud Platform (GCP) or Amazon Web Services (AWS), and is built using the Django web framework in Docker containers with a PostgreSQL database.

This guide will walk you through the deployment process for the Data Submission Tracker, including setting up the necessary environment variables, deploying the Django server, and configuring the database. Currently, the tool is deployed to an AWS EC2 instance; we will document the steps for deploying to AWS. If other cloud platforms are used we will need to modify this guide to reflect the changes.

## Getting started
See the CONTRIBUTING.md file for information on how to set up the repository for development and install the prerequisites. 

## Deployment Instructions


# Old Deployment Instructions

### Environment variables
For local development, the `api` directory should have an `.env` file with the following:

| name                    | value      | description                                                   |
| ----------------------- | ---------- | ------------------------------------------------------------- |
| DEBUG                   | `False`    | Set to `True` for local dev and `False` for prod              |
| DJANGO_LOG_LEVEL        |            | Set to `DEBUG` for local dev and `INFO` for prod              |
| SECRET_KEY              |            | The `SECRET_KEY` generated by Django                          |
| POSTGRES_DB             | `postgres` | The Postgres database name                                    |
| POSTGRES_USER           |            | The username of the Postgres User                             |
| POSTGRES_PASSWORD       |            | A (secure) password for the Postgres User                     |
| POSTGRES_HOST           |            | The external IP for the Compute Engine instance with Postgres |
| POSTGRES_PORT           | `5432`     | The port for the Postgres Database                            |

### The Postgres Database
In order to run, the Django tool-chain must have access to a Postgres database to manage stored information. For local development, the docker-compose-up.sh script will start a Postgres container with the necessary configuration. Once the postgres container is running, you will need to log into the app container and create a superuser to access the admin panel. The superuser can be created by running the following command in the app container:
```
docker exec -it bdcat-data-tracker_app /bin/bash
python manage.py createsuperuser
```

For deployment, the Postgres database my require addiational or manual configuration. The Postgres database must be accessible to the Django tool-chain via the `POSTGRES_HOST` environment variable.

### The Django Server

Docker Compose should start a Django server on [`http://0.0.0.0:8000/`](http://0.0.0.0:8000/).
The server uses the `.env` file for configuration

### Admin Panel

NHLBI Admins are able to view all tickets, but Data Custodians are only able to view their own tickets.
In the code, Admins are `staff`.
To access the Admin panel visit [`http://localhost:8000/admin`](http://localhost:8000/admin) as a `superuser` member

Go to the "Tracker/User" panel and select the user you want to grant admin permissions.
You should only need to check the `Is staff` permission for HNLBI Admins.
If you would like to allow that user to access the Admin panel, check the `Is superuser` permission as well

> NOTE: Make sure you save your changes at the bottom

### Docker Compose
Docker is used to standardize the development environment. For automatic setup locally, run the following command in the root directory:

```commandline
bash docker-compose-up.sh
```
You can also run `./docker-compose-up.sh`.

To setup the app manually, run the following commands in the `api` directory:

```commandline
docker-compose up --build
```

> NOTE: This may take a several minutes

You should only need to build this once (or when you make changes to the `Dockerfile` or `docker-compose.yml`).
Any subsequent runs do not require the `--build` flag:

To take down the containers, run the following command in the root directory:

```commandline
bash docker-compose-down.sh
```
You can also run `./docker-compose-down.sh`.

Alternatively, each container can be stopped individually:

```commandline
docker stop bdcat-data-tracker_app
docker stop bdcat-data-tracker_db
```

## Deployment

#### GitHub Secrets

For your convenience, this repo contains automatic deployment scripts.
These scripts will run depending on different triggers as detailed in the [`.github/workflows/`](.github/workflows/) directory.
To utilize these scripts, you will need to create a GitHub Secret with the following:

| name                 | description                             |
| -------------------- | --------------------------------------- |
| SECRET_KEY           | The `SECRET_KEY` generated by Django    |
| QUAY_NIMBUS_USERNAME | A username with access to the Quay repo |
| QUAY_NIMBUS_PASSWORD | The password of the Quay user           |

> NOTE: You can read more about this in the [Quay section below](#Quay)

### Instructions

**Ensure all prior setup is complete before continuing**

To prepare the project for deployment, run the following commands in the `api` directory:

```
python manage.py collectstatic --noinput
python manage.py makemigrations tracker
python manage.py migrate
```

> NOTE: The `collectstatic` command is not required if you are using the GitHub Actions workflow



# Everything below this line is for the old deployment method and is only here for reference

#### Quay

This repo has been configured to use GitHub Actions to build and push images to Quay on pushes to the `main` branch.
Specifically, the `Dockerfile` in the `api` directory will be used to build the image.
The image will be pushed to the [`nimbusinformatics/bdcat-data-tracker`](https://quay.io/repository/nimbusinformatics/bdcat-data-tracker) repository on Quay.
The image will be named `quay.io/nimbusinformatics/bdcat-data-tracker` and two tags: `latest` and a shortened commit hash.
Be sure to include the following in your GitHub Secrets:

| name                 | description                             |
| -------------------- | --------------------------------------- |
| QUAY_NIMBUS_USERNAME | A username with access to the Quay repo |
| QUAY_NIMBUS_PASSWORD | The password of the Quay user           |

> NOTE: Robot accounts are the preferred method of pushing images to Quay.
> These accounts are usually in the format: `<repo-name+<robot-name>`

##### Testing the Image

You can test the image locally by pulling the image from Quay

```
docker pull quay.io/nimbusinformatics/bdcat-data-tracker:latest
```

> NOTE: You will need a RedHat account with the correct permissions to pull the image

When running the image, you must bind a port to the container to access the API

```
docker run -p 8000:8000 quay.io/nimbusinformatics/bdcat-data-tracker:latest
```

A more detailed writeup can be found on the [Quay repository](https://quay.io/repository/nimbusinformatics/bdcat-data-tracker?tab=info)

#### Microsoft Azure

Azure must be set up manually.
You can find a detailed writeup in the [`azure`](/azure) directory

#### Google Cloud

We will be using [this guide to deploy to App Engine](https://cloud.google.com/python/django/appengine#macos-64-bit)

##### [Permissions](https://cloud.google.com/iam/docs/understanding-roles#predefined)

- [`roles/appengine.appAdmin`](https://cloud.google.com/iam/docs/understanding-roles#app-engine-roles)
- [`roles/cloudbuild.integrationsOwner`](https://cloud.google.com/iam/docs/understanding-roles#cloud-build-roles)
- [`roles/cloudbuild.builds.editor`](https://cloud.google.com/build/docs/iam-roles-permissions#predefined_roles)
- [`roles/secretmanager.admin`](https://cloud.google.com/iam/docs/understanding-roles#secret-manager-roles)
- [`roles/iam.serviceAccountAdmin`](https://cloud.google.com/iam/docs/understanding-roles#service-accounts-roles)
- [`roles/serviceusage.serviceUsageAdmin`](https://cloud.google.com/iam/docs/understanding-roles#service-usage-roles)
- [`roles/storage.admin`](https://cloud.google.com/iam/docs/understanding-roles#cloud-storage-roles)

> NOTE: You will also need to grant yourself the [`roles/iam.serviceAccountUser`](https://cloud.google.com/iam/docs/understanding-roles#service-accounts-roles) on the `App Engine default service account`

##### [Secrets Manager](https://cloud.google.com/python/django/appengine#create-django-environment-file-as-a-secret)

In GCP Secret Manager, you must create a secret called `django_settings`.
You can either upload the `.env` file or paste the secrets in there

> NOTE: If the Compute Engine instance restarts and the IP changes, you must update the `POSTGRES_HOST` variable

##### App Engine

This repo has been configured to use GitHub Actions to deploy to App Engine on pushes to the `main` branch.
For this to work, you must have GitHub Secrets with the following:

| name       | description                                         |
| ---------- | --------------------------------------------------- |
| PROJECT_ID | The Project ID for GCP                              |
| GCP_SA_KEY | The full service account key json exported from GCP |

Alternatively, can navigate to the `api` directory and run:

```
gcloud app deploy
```

#### SendGrid

You will also need to create your own dynamic templates and copy the `TEMPLATE_ID` and assign them into the correct variables in the [`mail.py`](/api/tracker/templates) file

The dynamic templates use handlebars syntax.
These must be included in the dynamic template you make on SendGrid.
These variables are listed under the `self.dynamic_template_data` dictionary of the [`mail.py`](/api/tracker/mail.py) file.
Some templates have been provided for you, but you must create your own if you want to make edits

> NOTE: You will need to [verify the sender identity in SendGrid]("https://docs.sendgrid.com/for-developers/sending-email/sender-identity") for the `SENDGRID_ADMIN_EMAIL`
